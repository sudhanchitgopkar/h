**High Performance Computing**
                Sudhan Chitgopkar, Harvard University
                `sudhanchitgopkar[at]g.harvard.edu`

# Lecture 1
## Moore's Law
- Integrated circuit (IC) resources double every 18-24 months
    - Every two years, you get a 2x performance boost wihtout changing your code
    - Originally thought this would be a result of decreasing transistor size, but transistors have already gotten extremely small
- *Can this keep going?*
- Two alternatives to satisfy Moore's law as transistor density reaches limit
    - Increase clock rate
    - Operate at power wall but increase number of processor cores
        - Modern Moore's Law, double number of cores per microprocessor generation every two years and we need to deal with the troubles of parallel computing

**The Power Wall**
- *capacitive load*: related to the number of transistors on the chip
- *switch rate*: related to the clock cycle of the chip
- Possible to decrease power consumption proportional to clock rate increase by reducing voltage
    - However, reducing voltage makes transistors leaky, which dissapates energy (currently the cause for 40% of power leakage)

## Serial Computing
- Series of instructions processed sequentially
- Executed on a single processor, with only one instruction executed at any given time

## Parallel Computing
- Problem divided into discrete chunks that can be solved concurrently
- Each chunk processed sequentially but run simultaneously on different processors
- Requires a *control/communication/coordination mechanism*
- Both data structures and algorithms must be suitable for dividing a problem into discrete chunks

## Terminology
- *Node*: a single entity with multiple processors like CPUs, GPUs, TPUs, etc (e.g. laptop, desktop, Raspberry Pi)
- *Cluster*: Arbitrary number of nodes connected by a local network
- *Task*: A logically discrete section of computational work. Can be thought of as a program (or program-like). 
    - Parallel programs consist of multiple tasks running m
- *Socket*: Mounting location of a CPU onto a motherboard
- *Pipelining*: Breaking a task into steps processed by different processor units
- *Shared memory*: All processors have direct access to common physical memory (all parallel tasks have the same picture of memory)
- *Symmetric Multi-processor (SMP)*: Shared memory and address space across multiple processors
- *Distributed memory*: tasks can only "see" local machine memory and must communicate with memory on other machines to get their data
- *Communication*: the process by which parallel tasks exchange data
- *Synchronization*: coordination of parallel tasks in real time, associated with communication
- *Granularity*: amount of computation vs. communication
    - Fine-grained: small amounts of computation and lots of communication in between
    - Coarse-grained: large amounts of computation and less communication
- *Observed speedup*: speedup of parallelized code: $$S = \frac{T(1)}{T(p)} = \frac{\text{Wall-clock time of serial execution}}{\text{Wall-clock time of parallel execution}}$$
- *Parallel overhead*: The amount of time required to coordinate parallel tasks, includes:
    - Task start-up time
    - Synchronizations
    - Data communication
    - Termination time
- *Scalability*: Ability for a parallel system to demonstrate proportional increase in parallel speedup with adddition of more resources
    - Function of hardware, application algorithm, parallelization overhead, etc

## Flynn's Taxonomy
- Method of classifying parallel computers based on instruction stream and data stream
- *Single Instruction Single Data (SISD)*
    - One instruction stream is being acted on by the CPU during one clock cycle
    - Only one data stream is being used as the input during one clock cycle
- *Single Instruction Multiple Data (SIMD)*
    - All processing units execute the same instruction on a given clock cycle
    - But each processing unit can operate on a different data element
    - Good for specialized problems characterized by large amounts of regularity (graphics/image processing)
    - Synchronous and deterministic

- *Multiple Instruction Single Data (MISD)*
    - Each unit operates on data independently through different instructions
    - But a single data stream is fet into the multiple processing units
    - Few uses
- *Multiple Instruction Multiple Data (MIMD)*
    - Each processor may execute different instructions on a different data stream
    - Most popular type of parallel computer
    - Most modern supercomputers use this
!!! WARNING
    MISD not in scope of this course!

# Lecture 2: Hardware
## Hardware Organization
- *Buses*: Collection of electricul circuits that carry bytes of information between components
- *Words*: Transferred chunks of bytes
    - Word size is a system parameter
    - Most word sizes are either 4 bytes (32 bit) or 8 bytes (64 bit)
- *Bandwidth*: Maximum rate at which byte chunks can be transferred
- *Main memory*: Temporary storage which holds a program and the data it manipulates during execution
    - Consistes of DRAM (dynamic random access memory), dunamoc meaning that memory cells are refreshed periodically
- *Processor*: Interprets/executes instructions from main memory
    - At the core is a register called` the program counter
    - At any point in time, PC points to a machine instruction
    - Processor uses instruction set architecture (ISA), commonly RISC and CISC
- Use main memory, register file, and ALI
    - Register file consists of all registers

## Compilation
1. Pre-processor: comments removed, adds include files
2. Compiler: ranslates to assembly
3. Assembler: translates assembly code input  $\rightarrow$ machine language 
4. Linker: links all necessary programs/files into one to create executable binary

## Von Neumann Architecture
![Von Neumann Architecture](figs/2-vn_architecture.png)

- Bottleneck: separation of memory and processor
    - Latency: response time for information to arrive from initial request
    - Bandwidth: the throughput of a system

**Three main modifications to recent CPUs**

- Caches: Reduce processor-memory gap
    - L1, closest to ALU and smallest $\rightarrow$ L3, furthest from ALU and biggest
- Virtual memory: Helps cache between DRAM and persistent storage
- Low-level parallelism: *covered later on!*

## Memory
- Virtual memory: an abstraction that provides each process with the illusion that it has power over main memory
    - Each process sees *virtual address space*
    - Virtual memory protects address space of each process from corruption by other processes

# Lecture 3: Caches
![Von Neumann Architecture with Caches](figs/3-vn_cache.png)

- *Cache*: small on-chip memories
    - L1: Smallest and closest to ALU (32kB size, exlcusive to core)
        - L1i holds instructions exclusively
        - L1d holds data exclusively
    - L2: Intermediate after L1, often shared amongst few cores (256 - 512 kB)
    - L3: Last level cache (LLC) closest to DRAM. Shared among cores (4 - 30 MB)
- Latency is different across cache levels
    - As lookups in closer caches fail, we move to higher and higher caches (then eventually to memory)

!!! Tip Why is this important?
    Understanding how caching and data locality works allows us to optimize it and improve runtime!

## Locality
- Well written programs exhibit good locality and run fast
    - Programs with good locality reference data items that are spatially near each other (spatial locality) or were recently referenced (temporal locality)
- Let's look at an example
    ```c linenumbers
        int v[N];
        int sum;

        for (int i = 0; i < N; i++) {
            sum += v[i];
        } //for
    ```
    - `sum` has good temporal locality, but `v[i]` does *not* have good temporal locality because each value `v[i]` is new and hasn't been used recently before but `sum` *has* been recently used
- Let's consider another example:
    ```c linenumbers
        for (int loop = 0; loop < 10; loop++) {
            for (int i = 0; i < N; i++) {
                sum += v[i];
            } //for
        } //for
    ```
    [A]


### Spatial Locality
- References memory "close" to already-referenced memory
    ```c linenumbers
        int sum;
        for (int i = 0; i < N; i++) {
            sum += v[i];
        } //for
    ```
    - This has good spatial locality because every element is next to the previous one
- Strided memory access
    - Algorithms may not always be one-stride
    - Stride-$k$ patterns visit every $k$th element of a contiguous vector
- Higher dimensional arrays
    - All memory is a contiguous vector of bytes
    - In `C/C++`, row-major ordering is used
        - This means the `j` index (column) is faster

## Cache Terminology
- *Cache block/line*: Fixed packet that moves back and forth between cache levels/main memory
- *Cache hit*: Data requested by the processor that *is* found in level $k$ and read directly from there
- *Cache miss*: Data requested by processor that is *not* found at the level $k$ cache. It is fetched from $k + 1$ and some line is evicted from $k$ according to a rule (e.g. LRU)
- *Hit/miss rate*: Fraction of memory accesses that are cache hits/misses in the level $k$ cache
- *Miss penalty*: Time to replace a block in level $k$ with a block from $k + 1$

## Cache Lines & Misses
- When data element from a data structure is accessed, multiple coalesced elemnts are loaded at once, not just one
- Types of cache misses:
    - *Compulsory/cold misses*: inavoidable, occur when the cache is empty or when trying to access data we've never seen before
    - *Capacity misses*: when the cache is full so you can't access/store an element that you've previously seen
    - *Conflict misses*: Two elements compete for the same spot in the cache despite it not being full
- Cache line placement policies:
!!! ERROR ToDo
    Fill out notes from here onwards $\rightarrow$ conflict misses

# Lecture 4: Shared Memory
- When cores cooperate, they share/exhange data
    - Done by sharing a memory address space where multiple cores have read and write privileges
- Parallelism in applications:
    - *Task-level parallelism (TLP)*: Identifies tasks that can be executed in parallel. Shared memory programming belongs here.
    - *Data-level parallelism (DLP)*: Manhy data items exist which can be operated on simultaneously
    - *Instruction-level parallelism (ILP)*: Exploits *DLP*  through things like pipelining or speculative execution
    - *Vector architectures, GPIs, and vector instructions*: Exploits *DLP* by running same instruction to collection of data
    - *Thread level parallelism*: Exploits *DLP* or *TLP* in a hardware model that alloes for interaction between parallel threads
## Processes vs Threads
- Process
    - Is an execution sequence within OS
    - Owns a virtual address space
    - Has an execution state
    - Managed by OS
- Thread
    - Can execute a sequence of instructions within a running process
    - All threads with the same process share application data in main memory
    - Threads are cheaper to create than processes
    - Can be controlled by the developer
- Hardware threads
    - Help hide latency by splitting one physical core into two logical cores
    - Software threads are scheduled for execution on one logical core
    - Context switch between the two logical CPUs when one is idle/waiting for data

### Processes
- No process can access the virtual memory of another process
- Communication between processes, though, is important
    - This is implemented by the OS with pipes, sockets, etc/

### Threads
- Independent stream of instructions scheduled to run by OS
- Processes can create *many threads*
- Each thread:
    - Shares memory of the process they belong to with the other threads
    - Has its own state and private memory on the stack
    - Cheap to create

### Race Conditions
- With shared memory, multiple trajectories are possible and **all** must yield the correct result

!!! ERROR ToDo
    Review "Race Condition in `Counter` Class"

# Lecture 5

!!! TIP Identifying Race Conditions
    Ask – should there be a specific order among the `read` and `write` operations in code that is executed by multiple threads in parallel

- Compilers don't enforce ordering during race conditions because this sacrifices performance and optimization
- Solution: use a memory consistency model that defines some *synchronization primitives* which dictate allowde/disallowed optimization
!!! WARNING Synchronization Primitives
    Synchronization means serialization! Use of syncrhonization primitives is expensive and hurts the parallelizability of your code.

## Memory Consistency Model
- The result of any execution is the same as if it was done sequentially (*total order*), and the operations of each individual process appear in the sequence specified by its program (*partial order*)
- The *critical section* of code is the time between acquiring and releasing exclusive access to some data

### Sequential Model
- Read/write order is consistent with program order
- Easiest and most intuitive but too restrictive for compiler optimization and not widely used

### Relaxed Consistency Models
- **Maintaining sequential consistency is expensive**
- We can relax the rigorous sequential requirement and allow `read` and `write` operations to complete out of order
- This requires explicit synchronization operations to enforce sequential consistency

## Mutual Exclusion
- Sometimes, we need to control access to critical sections. A `lock` (sometimes called `mutex`) is an abstract data type that implements mutual exclsion
- *Mutual exclusion*: critical sections of different threads don't overlap
- *Memory consistent*: operations are visible to all threads after the critical section
- *Progress*: If a thread is not inside the critical section, another thread can always enter
- *Starvation-freedom* (or deadlock-freedom): if a thread wants to enter the critical section, it will eventually succeed

# Lecture 6: `OpenMP`
- `OpenMP`: Open Multi-processing
    - Compiler directives
    - Runtime lib calls
    - Environment variables


<!-- Markdeep: -->
<head>
<link rel="stylesheet" href="https://morgan3d.github.io/markdeep/latest/latex.css?">

<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://morgan3d.github.io/markdeep/latest/markdeep.min.js" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>
</head>`